package agent

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"log/slog"
	"strings"

	"gomentum/internal/config"
	gmcp "gomentum/internal/mcp"
	"gomentum/internal/planner"

	"github.com/mark3labs/mcp-go/mcp"
	openai "github.com/sashabaranov/go-openai"
)

// Agent defines the interface for our planning agent
type Agent interface {
	// Chat sends a message to the agent and returns the response
	// onToken is called for each token generated by the LLM
	Chat(ctx context.Context, prompt string, onToken func(string)) (string, error)
}

// OpenAIAgent implements Agent for OpenAI-compatible APIs (e.g., DeepSeek)
type OpenAIAgent struct {
	client    *openai.Client
	cfg       *config.Config
	mcpServer *gmcp.Server
	planner   *planner.Planner
	history   []openai.ChatCompletionMessage // In-memory history including tool calls
}

// NewAgent creates a new agent
func NewAgent(cfg *config.Config, mcpServer *gmcp.Server, p *planner.Planner) (Agent, error) {
	clientConfig := openai.DefaultConfig(cfg.LLM.APIKey)
	clientConfig.BaseURL = cfg.LLM.BaseURL

	client := openai.NewClientWithConfig(clientConfig)

	agent := &OpenAIAgent{
		client:    client,
		cfg:       cfg,
		mcpServer: mcpServer,
		planner:   p,
		history:   []openai.ChatCompletionMessage{},
	}

	// Load history from DB
	if err := agent.loadHistory(); err != nil {
		slog.Warn("Failed to load chat history", "error", err)
	}

	// Ensure system prompt is present (will be updated in Chat)
	if len(agent.history) == 0 || agent.history[0].Role != openai.ChatMessageRoleSystem {
		agent.history = append([]openai.ChatCompletionMessage{{
			Role:    openai.ChatMessageRoleSystem,
			Content: "You are Gomentum, a helpful planning assistant.",
		}}, agent.history...)
	}

	return agent, nil
}

func (a *OpenAIAgent) loadHistory() error {
	messages, err := a.planner.GetRecentMessages(a.cfg.Agent.MaxHistory)
	if err != nil {
		return err
	}

	for _, m := range messages {
		a.history = append(a.history, openai.ChatCompletionMessage{
			Role:    m.Role,
			Content: m.Content,
		})
	}
	return nil
}

// Chat implements the Agent interface
func (a *OpenAIAgent) Chat(ctx context.Context, prompt string, onToken func(string)) (string, error) {
	// Static system prompt: force live time from tool, never cached clock
	systemPrompt := "You are Gomentum, a helpful planning assistant. ALWAYS call the tool `current_time` before any time reasoning or scheduling to get the freshest local timestamp (RFC3339 with offset). Treat the latest `current_time` result as the only authoritative 'now' and ignore any earlier timestamps in the conversation. When calling tools with start_time or end_time, use RFC3339 with the SAME timezone offset as the current time; do not convert to UTC. If the user provides a relative time (like 'tomorrow', 'next Monday'), first call `current_time`, then calculate the absolute date and EXECUTE the scheduling tool immediately. Do not ask for confirmation unless the time is ambiguous. Be concise."

	if len(a.history) > 0 && a.history[0].Role == openai.ChatMessageRoleSystem {
		a.history[0].Content = systemPrompt
	} else {
		a.history = append([]openai.ChatCompletionMessage{{
			Role:    openai.ChatMessageRoleSystem,
			Content: systemPrompt,
		}}, a.history...)
	}

	// Add user message to history and DB
	a.history = append(a.history, openai.ChatCompletionMessage{
		Role:    openai.ChatMessageRoleUser,
		Content: prompt,
	})
	if err := a.planner.SaveMessage(openai.ChatMessageRoleUser, prompt); err != nil {
		slog.Error("Failed to save user message", "error", err)
	}

	// Always inject a fresh current_time reading before reasoning
	a.addCurrentTimeSnapshot(ctx, systemPrompt)

	// Remove stale time-bearing messages from prior turns to avoid the model echoing old timestamps
	a.pruneStaleTimeMessages()

	// Prepare tools
	tools := a.getOpenAITools()

	// Loop to handle tool calls
	// Safety: Limit max iterations to prevent infinite loops
	maxIterations := 10
	for i := 0; i < maxIterations; i++ {
		// Sliding Window: Select messages for context
		contextMessages := a.getContextMessages()

		stream, err := a.client.CreateChatCompletionStream(
			ctx,
			openai.ChatCompletionRequest{
				Model:    a.cfg.LLM.Model,
				Messages: contextMessages,
				Tools:    tools,
				Stream:   true,
			},
		)
		if err != nil {
			return "", err
		}
		defer stream.Close()

		var (
			fullContent string
			toolCalls   []openai.ToolCall
		)

		// Stream loop
		for {
			response, err := stream.Recv()
			if errors.Is(err, io.EOF) {
				break
			}
			if err != nil {
				return "", fmt.Errorf("stream error: %v", err)
			}

			if len(response.Choices) == 0 {
				continue
			}

			delta := response.Choices[0].Delta

			// Handle content delta
			if delta.Content != "" {
				fullContent += delta.Content
				if onToken != nil {
					onToken(delta.Content)
				}
			}

			// Handle tool calls delta
			// Note: Tool calls are streamed in parts. We need to accumulate them.
			// The go-openai library's Delta.ToolCalls usually contains the index and partial data.
			for _, tc := range delta.ToolCalls {
				// Ensure slice is large enough
				if tc.Index != nil {
					idx := *tc.Index
					for len(toolCalls) <= idx {
						toolCalls = append(toolCalls, openai.ToolCall{})
					}
					// Update ID
					if tc.ID != "" {
						toolCalls[idx].ID = tc.ID
						toolCalls[idx].Type = tc.Type
					}
					// Update Function Name
					if tc.Function.Name != "" {
						if toolCalls[idx].Function.Name == "" {
							toolCalls[idx].Function.Name = tc.Function.Name
						} else {
							toolCalls[idx].Function.Name += tc.Function.Name
						}
					}
					// Update Function Arguments
					if tc.Function.Arguments != "" {
						toolCalls[idx].Function.Arguments += tc.Function.Arguments
					}
				}
			}
		}
		stream.Close()

		// Construct the full message
		msg := openai.ChatCompletionMessage{
			Role:      openai.ChatMessageRoleAssistant,
			Content:   fullContent,
			ToolCalls: toolCalls,
		}
		a.history = append(a.history, msg)

		// If there are no tool calls, we are done
		if len(toolCalls) == 0 {
			// Save assistant response to DB
			if err := a.planner.SaveMessage(openai.ChatMessageRoleAssistant, fullContent); err != nil {
				slog.Error("Failed to save assistant message", "error", err)
			}
			return fullContent, nil
		}

		// Handle tool calls
		for _, toolCall := range toolCalls {
			slog.Info("Calling tool", "tool", toolCall.Function.Name)
			// Visual feedback for tool calls (since we are streaming, we might want to print a newline first)
			if onToken != nil {
				onToken(fmt.Sprintf("\n  > Executing %s...\n", toolCall.Function.Name))
			}

			var args map[string]interface{}
			if err := json.Unmarshal([]byte(toolCall.Function.Arguments), &args); err != nil {
				content := fmt.Sprintf("Error parsing arguments: %v", err)
				a.history = append(a.history, openai.ChatCompletionMessage{
					Role:       openai.ChatMessageRoleTool,
					Content:    content,
					ToolCallID: toolCall.ID,
				})
				continue
			}

			result, err := a.mcpServer.CallTool(ctx, toolCall.Function.Name, args)
			content := ""
			if err != nil {
				content = fmt.Sprintf("Error: %v", err)
			} else {
				for _, c := range result.Content {
					if textContent, ok := c.(mcp.TextContent); ok {
						content += textContent.Text + "\n"
					}
				}
			}

			a.history = append(a.history, openai.ChatCompletionMessage{
				Role:       openai.ChatMessageRoleTool,
				Content:    content,
				ToolCallID: toolCall.ID,
			})
		}
		// Loop continues to send tool results back to LLM
	}

	return "", fmt.Errorf("max iterations reached")
}

func (a *OpenAIAgent) getContextMessages() []openai.ChatCompletionMessage {
	// Always include system prompt
	if len(a.history) == 0 {
		return []openai.ChatCompletionMessage{}
	}

	systemMsg := a.history[0]
	remaining := a.history[1:]

	maxHistory := a.cfg.Agent.MaxHistory
	if len(remaining) > maxHistory {
		remaining = remaining[len(remaining)-maxHistory:]
	}

	// Reconstruct
	msgs := append([]openai.ChatCompletionMessage{systemMsg}, remaining...)
	return msgs
}

func (a *OpenAIAgent) getOpenAITools() []openai.Tool {
	mcpTools := a.mcpServer.GetTools()
	var tools []openai.Tool

	for _, t := range mcpTools {
		tools = append(tools, openai.Tool{
			Type: openai.ToolTypeFunction,
			Function: &openai.FunctionDefinition{
				Name:        t.Name,
				Description: t.Description,
				Parameters:  t.InputSchema,
			},
		})
	}
	return tools
}

// pruneStaleTimeMessages removes older assistant/system/tool messages that contain time statements,
// so the model doesn't reuse outdated timestamps. The first system message is always kept.
func (a *OpenAIAgent) pruneStaleTimeMessages() {
	if len(a.history) == 0 {
		return
	}
	systemMsg := a.history[0]
	var filtered []openai.ChatCompletionMessage
	filtered = append(filtered, systemMsg)

	for _, msg := range a.history[1:] {
		if msg.Role == openai.ChatMessageRoleAssistant || msg.Role == openai.ChatMessageRoleSystem || msg.Role == openai.ChatMessageRoleTool {
			text := strings.ToLower(msg.Content)
			if strings.Contains(text, "current_time") ||
				strings.Contains(text, "current time") ||
				strings.Contains(text, "local time") ||
				strings.Contains(msg.Content, "根据系统时间") ||
				strings.Contains(msg.Content, "当前时间") {
				continue
			}
		}
		filtered = append(filtered, msg)
	}
	a.history = filtered
}

// addCurrentTimeSnapshot calls the MCP current_time tool and appends the result as a system message
// so the model always sees the freshest time before responding.
func (a *OpenAIAgent) addCurrentTimeSnapshot(ctx context.Context, baseSystemPrompt string) {
	result, err := a.mcpServer.CallTool(ctx, "current_time", map[string]interface{}{})
	if err != nil || result == nil {
		slog.Warn("current_time tool failed", "error", err)
		return
	}

	var content string
	for _, c := range result.Content {
		if textContent, ok := c.(mcp.TextContent); ok {
			content += textContent.Text
		}
	}
	if content == "" {
		return
	}

	// Replace/augment the system prompt with the live time
	combined := fmt.Sprintf("%s Latest current_time result: %s", baseSystemPrompt, content)
	if len(a.history) > 0 && a.history[0].Role == openai.ChatMessageRoleSystem {
		a.history[0].Content = combined
	} else {
		a.history = append([]openai.ChatCompletionMessage{{
			Role:    openai.ChatMessageRoleSystem,
			Content: combined,
		}}, a.history...)
	}
}
