package agent

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"log/slog"
	"strings"
	"time"

	"gomentum/internal/config"
	gmcp "gomentum/internal/mcp"
	"gomentum/internal/planner"

	"github.com/mark3labs/mcp-go/mcp"
	openai "github.com/sashabaranov/go-openai"
)

// Agent defines the interface for our planning agent
type Agent interface {
	// Chat sends a message to the agent and returns the response
	// onToken is called for each token generated by the LLM
	Chat(ctx context.Context, prompt string, onToken func(string)) (string, error)
}

// OpenAIAgent implements Agent for OpenAI-compatible APIs (e.g., DeepSeek)
type OpenAIAgent struct {
	client    *openai.Client
	cfg       *config.Config
	mcpServer *gmcp.Server
	planner   *planner.Planner
	history   []openai.ChatCompletionMessage // In-memory history including tool calls
}

// NewAgent creates a new agent
func NewAgent(cfg *config.Config, mcpServer *gmcp.Server, p *planner.Planner) (Agent, error) {
	clientConfig := openai.DefaultConfig(cfg.LLM.APIKey)
	clientConfig.BaseURL = cfg.LLM.BaseURL

	client := openai.NewClientWithConfig(clientConfig)

	agent := &OpenAIAgent{
		client:    client,
		cfg:       cfg,
		mcpServer: mcpServer,
		planner:   p,
		history:   []openai.ChatCompletionMessage{},
	}

	// Load history from DB
	if err := agent.loadHistory(); err != nil {
		slog.Warn("Failed to load chat history", "error", err)
	}

	// Ensure system prompt is present (will be updated in Chat)
	if len(agent.history) == 0 || agent.history[0].Role != openai.ChatMessageRoleSystem {
		agent.history = append([]openai.ChatCompletionMessage{{
			Role:    openai.ChatMessageRoleSystem,
			Content: "You are Gomentum, a helpful planning assistant.",
		}}, agent.history...)
	}

	return agent, nil
}

func (a *OpenAIAgent) loadHistory() error {
	messages, err := a.planner.GetRecentMessages(a.cfg.Agent.MaxHistory)
	if err != nil {
		return err
	}

	for _, m := range messages {
		a.history = append(a.history, openai.ChatCompletionMessage{
			Role:    m.Role,
			Content: m.Content,
		})
	}
	return nil
}

// Chat implements the Agent interface
func (a *OpenAIAgent) Chat(ctx context.Context, prompt string, onToken func(string)) (string, error) {
	// Static system prompt: force live time from tool, never cached clock
	systemPrompt := "You are Gomentum, a helpful planning assistant. ALWAYS call the tool `current_time` before any time reasoning or scheduling to get the freshest local timestamp (RFC3339 with offset). Treat the latest `current_time` result as the only authoritative 'now' and ignore any earlier timestamps in the conversation. When calling tools with start_time or end_time, use RFC3339 with the SAME timezone offset as the current time; do not convert to UTC. If the user provides a relative time (like 'tomorrow', 'next Monday'), first call `current_time`, then calculate the absolute date and EXECUTE the scheduling tool immediately. Do not ask for confirmation unless the time is ambiguous. Be concise."

	if len(a.history) > 0 && a.history[0].Role == openai.ChatMessageRoleSystem {
		a.history[0].Content = systemPrompt
	} else {
		a.history = append([]openai.ChatCompletionMessage{{
			Role:    openai.ChatMessageRoleSystem,
			Content: systemPrompt,
		}}, a.history...)
	}

	// Add user message to history and DB
	a.history = append(a.history, openai.ChatCompletionMessage{
		Role:    openai.ChatMessageRoleUser,
		Content: prompt,
	})
	if err := a.planner.SaveMessage(openai.ChatMessageRoleUser, prompt); err != nil {
		slog.Error("Failed to save user message", "error", err)
	}

	// Always inject a fresh current_time tool call/result before reasoning
	a.ensureCurrentTimeToolCall(ctx, systemPrompt, onToken)

	// Remove stale time-bearing messages from prior turns to avoid the model echoing old timestamps
	a.pruneStaleTimeMessages()

	// Prepare tools
	tools := a.getOpenAITools()

	// Loop to handle tool calls
	// Safety: Limit max iterations to prevent infinite loops
	maxIterations := 10
	for i := 0; i < maxIterations; i++ {
		// Sliding Window: Select messages for context
		contextMessages := a.getContextMessages()

		stream, err := a.client.CreateChatCompletionStream(
			ctx,
			openai.ChatCompletionRequest{
				Model:    a.cfg.LLM.Model,
				Messages: contextMessages,
				Tools:    tools,
				Stream:   true,
			},
		)
		if err != nil {
			return "", err
		}
		defer stream.Close()

		var (
			fullContent string
			toolCalls   []openai.ToolCall
		)

		// Stream loop
		for {
			response, err := stream.Recv()
			if errors.Is(err, io.EOF) {
				break
			}
			if err != nil {
				return "", fmt.Errorf("stream error: %v", err)
			}

			if len(response.Choices) == 0 {
				continue
			}

			delta := response.Choices[0].Delta

			// Handle content delta
			if delta.Content != "" {
				fullContent += delta.Content
				if onToken != nil {
					onToken(delta.Content)
				}
			}

			// Handle tool calls delta
			// Note: Tool calls are streamed in parts. We need to accumulate them.
			// The go-openai library's Delta.ToolCalls usually contains the index and partial data.
			for _, tc := range delta.ToolCalls {
				// Ensure slice is large enough
				if tc.Index != nil {
					idx := *tc.Index
					for len(toolCalls) <= idx {
						toolCalls = append(toolCalls, openai.ToolCall{})
					}
					// Update ID
					if tc.ID != "" {
						toolCalls[idx].ID = tc.ID
						toolCalls[idx].Type = tc.Type
					}
					// Update Function Name
					if tc.Function.Name != "" {
						if toolCalls[idx].Function.Name == "" {
							toolCalls[idx].Function.Name = tc.Function.Name
						} else {
							toolCalls[idx].Function.Name += tc.Function.Name
						}
					}
					// Update Function Arguments
					if tc.Function.Arguments != "" {
						toolCalls[idx].Function.Arguments += tc.Function.Arguments
					}
				}
			}
		}
		stream.Close()

		// Construct the full message
		msg := openai.ChatCompletionMessage{
			Role:      openai.ChatMessageRoleAssistant,
			Content:   fullContent,
			ToolCalls: toolCalls,
		}
		a.history = append(a.history, msg)

		// If there are no tool calls, we are done
		if len(toolCalls) == 0 {
			// Save assistant response to DB
			if err := a.planner.SaveMessage(openai.ChatMessageRoleAssistant, fullContent); err != nil {
				slog.Error("Failed to save assistant message", "error", err)
			}
			return fullContent, nil
		}

		// Handle tool calls
		for _, toolCall := range toolCalls {
			slog.Info("Calling tool", "tool", toolCall.Function.Name)
			// Visual feedback for tool calls (since we are streaming, we might want to print a newline first)
			if onToken != nil {
				onToken(fmt.Sprintf("\n  > Executing %s...\n", toolCall.Function.Name))
			}

			var args map[string]interface{}
			if err := json.Unmarshal([]byte(toolCall.Function.Arguments), &args); err != nil {
				content := fmt.Sprintf("Error parsing arguments: %v", err)
				a.history = append(a.history, openai.ChatCompletionMessage{
					Role:       openai.ChatMessageRoleTool,
					Content:    content,
					ToolCallID: toolCall.ID,
				})
				continue
			}

			result, err := a.mcpServer.CallTool(ctx, toolCall.Function.Name, args)
			content := ""
			if err != nil {
				content = fmt.Sprintf("Error: %v", err)
			} else {
				for _, c := range result.Content {
					if textContent, ok := c.(mcp.TextContent); ok {
						content += textContent.Text + "\n"
					}
				}
			}

			a.history = append(a.history, openai.ChatCompletionMessage{
				Role:       openai.ChatMessageRoleTool,
				Content:    content,
				ToolCallID: toolCall.ID,
			})
		}
		// Loop continues to send tool results back to LLM
	}

	return "", fmt.Errorf("max iterations reached")
}

func (a *OpenAIAgent) getContextMessages() []openai.ChatCompletionMessage {
	// Always include system prompt
	if len(a.history) == 0 {
		return []openai.ChatCompletionMessage{}
	}

	systemMsg := a.history[0]
	remaining := a.history[1:]

	maxHistory := a.cfg.Agent.MaxHistory
	if len(remaining) > maxHistory {
		remaining = remaining[len(remaining)-maxHistory:]
	}

	// Reconstruct
	msgs := append([]openai.ChatCompletionMessage{systemMsg}, remaining...)
	return ensureToolCallConsistency(msgs)
}

// ensureCurrentTimeToolCall makes a synthetic tool_call for current_time and stores its result,
// so the model always has a live timestamp and the UI can display the tool call/response.
func (a *OpenAIAgent) ensureCurrentTimeToolCall(ctx context.Context, baseSystemPrompt string, onToken func(string)) {
	// Avoid duplicate within the last few messages
	for i := len(a.history) - 1; i >= 0 && len(a.history)-i <= 6; i-- {
		msg := a.history[i]
		if msg.Role == openai.ChatMessageRoleAssistant {
			for _, tc := range msg.ToolCalls {
				if tc.Function.Name == "current_time" {
					return
				}
			}
		}
	}

	callID := fmt.Sprintf("auto_current_time_%d", time.Now().UnixNano())
	toolCall := openai.ToolCall{
		ID:   callID,
		Type: openai.ToolTypeFunction,
		Function: openai.FunctionCall{
			Name:      "current_time",
			Arguments: "{}",
		},
	}

	// Append assistant tool call
	a.history = append(a.history, openai.ChatCompletionMessage{
		Role:      openai.ChatMessageRoleAssistant,
		ToolCalls: []openai.ToolCall{toolCall},
	})
	if onToken != nil {
		onToken(fmt.Sprintf("\n| Executing %s...\n", toolCall.Function.Name))
	}

	// Execute tool
	result, err := a.mcpServer.CallTool(ctx, "current_time", map[string]interface{}{})
	content := ""
	if err != nil || result == nil {
		content = fmt.Sprintf("current_time tool failed: %v", err)
	} else {
		for _, c := range result.Content {
			if textContent, ok := c.(mcp.TextContent); ok {
				content += textContent.Text
			}
		}
		if content == "" {
			content = "current_time tool returned empty content"
		}
	}
	if onToken != nil {
		onToken(fmt.Sprintf("| %s result: %s\n", toolCall.Function.Name, content))
	}

	// Append tool response
	a.history = append(a.history, openai.ChatCompletionMessage{
		Role:       openai.ChatMessageRoleTool,
		ToolCallID: callID,
		Content:    content,
	})

	// Update system prompt with latest time context
	combined := fmt.Sprintf("%s Latest current_time result: %s", baseSystemPrompt, content)
	if len(a.history) > 0 && a.history[0].Role == openai.ChatMessageRoleSystem {
		a.history[0].Content = combined
	}
}

func (a *OpenAIAgent) getOpenAITools() []openai.Tool {
	mcpTools := a.mcpServer.GetTools()
	var tools []openai.Tool

	for _, t := range mcpTools {
		tools = append(tools, openai.Tool{
			Type: openai.ToolTypeFunction,
			Function: &openai.FunctionDefinition{
				Name:        t.Name,
				Description: t.Description,
				Parameters:  t.InputSchema,
			},
		})
	}
	return tools
}

// pruneStaleTimeMessages keeps only the latest current_time tool call/response
// and drops older time-bearing messages. The first system message is always kept.
func (a *OpenAIAgent) pruneStaleTimeMessages() {
	if len(a.history) == 0 {
		return
	}
	systemMsg := a.history[0]
	var filtered []openai.ChatCompletionMessage
	filtered = append(filtered, systemMsg)

	// Find the most recent current_time call ID (assistant tool call or tool response)
	lastTimeCallID := ""
	for i := len(a.history) - 1; i >= 1; i-- {
		msg := a.history[i]
		if msg.Role == openai.ChatMessageRoleAssistant {
			for _, tc := range msg.ToolCalls {
				if tc.Function.Name == "current_time" {
					lastTimeCallID = tc.ID
					break
				}
			}
		}
		if msg.Role == openai.ChatMessageRoleTool && msg.ToolCallID != "" {
			if strings.Contains(msg.Content, "current_time") {
				lastTimeCallID = msg.ToolCallID
			}
		}
		if lastTimeCallID != "" {
			break
		}
	}

	for i := 1; i < len(a.history); i++ {
		msg := a.history[i]

		// Drop orphan tool messages (must follow an assistant with matching tool_call_id)
		if msg.Role == openai.ChatMessageRoleTool {
			if len(filtered) == 0 {
				continue
			}
			prev := filtered[len(filtered)-1]
			if prev.Role != openai.ChatMessageRoleAssistant {
				continue
			}
			match := false
			for _, tc := range prev.ToolCalls {
				if tc.ID == msg.ToolCallID {
					match = true
					break
				}
			}
			if !match {
				continue
			}
		}

		if isTimeMessage(msg) {
			// Keep only the latest current_time pair
			if msg.Role == openai.ChatMessageRoleAssistant {
				keep := false
				for _, tc := range msg.ToolCalls {
					if tc.ID == lastTimeCallID && tc.Function.Name == "current_time" {
						keep = true
						break
					}
				}
				if !keep {
					continue
				}
			} else if msg.Role == openai.ChatMessageRoleTool {
				if msg.ToolCallID != lastTimeCallID {
					continue
				}
			} else {
				// Drop older time-bearing system/assistant messages
				if lastTimeCallID != "" {
					continue
				}
			}
		}

		filtered = append(filtered, msg)
	}

	a.history = filtered
}

func isTimeMessage(msg openai.ChatCompletionMessage) bool {
	if msg.Role == openai.ChatMessageRoleAssistant {
		for _, tc := range msg.ToolCalls {
			if tc.Function.Name == "current_time" {
				return true
			}
		}
	}
	if msg.Role == openai.ChatMessageRoleSystem || msg.Role == openai.ChatMessageRoleTool || msg.Role == openai.ChatMessageRoleAssistant {
		text := strings.ToLower(msg.Content)
		if strings.Contains(text, "current_time") ||
			strings.Contains(text, "current time") ||
			strings.Contains(text, "local time") ||
			strings.Contains(msg.Content, "根据系统时间") ||
			strings.Contains(msg.Content, "当前时间") {
			return true
		}
	}
	return false
}

// ensureToolCallConsistency removes assistant messages that have tool_calls
// without matching tool responses in the subsequent messages, and drops orphan
// tool messages that don't correspond to a kept assistant message. This prevents
// OpenAI API 400 errors about mismatched tool_call/tool messages.
func ensureToolCallConsistency(msgs []openai.ChatCompletionMessage) []openai.ChatCompletionMessage {
	if len(msgs) == 0 {
		return msgs
	}

	// Backward scan: mark assistant messages that have all tool responses present later.
	seenTool := map[string]bool{}
	keep := make([]bool, len(msgs))
	for i := len(msgs) - 1; i >= 0; i-- {
		msg := msgs[i]
		switch msg.Role {
		case openai.ChatMessageRoleTool:
			if msg.ToolCallID != "" {
				seenTool[msg.ToolCallID] = true
			}
			// Tentatively keep; will validate in forward pass
			keep[i] = true
		case openai.ChatMessageRoleAssistant:
			if len(msg.ToolCalls) == 0 {
				keep[i] = true
				continue
			}
			ok := true
			for _, tc := range msg.ToolCalls {
				if !seenTool[tc.ID] {
					ok = false
					break
				}
			}
			keep[i] = ok
		default:
			keep[i] = true
		}
	}

	// Forward scan: drop orphan tool messages and unresolved assistants
	active := map[string]bool{}
	var fixed []openai.ChatCompletionMessage
	for i, msg := range msgs {
		if !keep[i] {
			continue
		}

		if msg.Role == openai.ChatMessageRoleAssistant && len(msg.ToolCalls) > 0 {
			for _, tc := range msg.ToolCalls {
				active[tc.ID] = true
			}
			fixed = append(fixed, msg)
			continue
		}

		if msg.Role == openai.ChatMessageRoleTool {
			if active[msg.ToolCallID] {
				fixed = append(fixed, msg)
				delete(active, msg.ToolCallID)
			}
			continue
		}

		fixed = append(fixed, msg)
	}

	return fixed
}

// addCurrentTimeSnapshot calls the MCP current_time tool and appends the result as a system message
// so the model always sees the freshest time before responding.
func (a *OpenAIAgent) addCurrentTimeSnapshot(ctx context.Context, baseSystemPrompt string) {
	result, err := a.mcpServer.CallTool(ctx, "current_time", map[string]interface{}{})
	if err != nil || result == nil {
		slog.Warn("current_time tool failed", "error", err)
		return
	}

	var content string
	for _, c := range result.Content {
		if textContent, ok := c.(mcp.TextContent); ok {
			content += textContent.Text
		}
	}
	if content == "" {
		return
	}

	// Replace/augment the system prompt with the live time
	combined := fmt.Sprintf("%s Latest current_time result: %s", baseSystemPrompt, content)
	if len(a.history) > 0 && a.history[0].Role == openai.ChatMessageRoleSystem {
		a.history[0].Content = combined
	} else {
		a.history = append([]openai.ChatCompletionMessage{{
			Role:    openai.ChatMessageRoleSystem,
			Content: combined,
		}}, a.history...)
	}
}
